{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d1dad79-1b60-4f9d-93fb-72e055ca20a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files removed: 40\n"
     ]
    }
   ],
   "source": [
    "!pip cache purge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d2786de-792c-4e18-8256-aff513a075ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip uninstall -y bitsandbytes\n",
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d2a13cc-af05-4de6-b714-697cf4630ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install datasets\n",
    "!pip install accelerate\n",
    "!pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed7e9401-ac0c-4d30-828d-69b8f24cbc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ca99b2d-bbde-40fb-a4a5-63c37fd53324",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --upgrade typing_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a69bbace-130a-49dc-93ff-7e6d1c0b27fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a283a63-0c0d-46f5-8d1f-ce3c3ae697ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4ce3b24-6015-4ac0-b476-2a53c819608b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "48d888e5-31b1-4310-b9e2-fbc176bae9a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa90a7039fff4293971101f83fc1e6d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"csv\", data_files='./train_augmented_2.csv', encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b8afa65a-06a0-468f-81b3-ac5b662d6ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['ID', 'input', 'output', 'restored', 'noised'],\n",
      "        num_rows: 11263\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bf6123a4-a3f5-48dd-a1bb-290124e5831c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(examples):\n",
    "  messages = [\n",
    "      f\"input : {examples['input']}, output : {examples['output']}\",\n",
    "      f\"input : {examples['restored']}, output : {examples['output']}\",\n",
    "      f\"input : {examples['noised']}, output : {examples['output']}\"\n",
    "  ]\n",
    "\n",
    "  prompt = \"\\n\".join([m for m in messages]).strip()\n",
    "\n",
    "  return {\"prompt\": prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1310fae8-8974-44d5-8b87-1830f192860b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0483b460ded410691d8be9c3f2057be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11263 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompted_datasets = dataset[\"train\"].map(\n",
    "    generate_prompt,\n",
    "    # remove_columns=dataset[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b41ffa90-7d58-45bd-9b11-4fb77fa37a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['ID', 'input', 'output', 'restored', 'noised', 'prompt'],\n",
      "    num_rows: 11263\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(prompted_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "41963221-3ec7-4f61-a1c4-5f6547d4f540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'input : 별 한 게토 았깝땀. 왜 싸람듯릭 펼 1캐를 쥰눈징 컥꺾폰 싸람믐롯섞 맒록 섧멍핥쟈닐 탯끎룐눈 녀뮤 퀼교... 야뭍툰 둠 변 닺씨 깍낄 싫훈 굣. 깸삥읊 20여 년 댜녁뵨 곧 중 쩨윌 귑푼 낙팠떤 곶., output : 별 한 개도 아깝다. 왜 사람들이 별 1개를 주는지 겪어본 사람으로서 말로 설명하자니 댓글로는 너무 길고... 아무튼 두 번 다시 가길 싫은 곳. 캠핑을 20여 년 다녀본 곳 중 제일 기분 나빴던 곳.\\ninput : 별 한 개도 아깝다. 왜 사람들이 별 1개를 주는지 경험한 사람으로서 말로 설명하자니 텍스트로는 너무 길고... 아무튼 두 번 다시 가기 싫은 곳. 캠핑을 20여 년 다녀본 곳 중 제일 기분 나빴던 곳., output : 별 한 개도 아깝다. 왜 사람들이 별 1개를 주는지 겪어본 사람으로서 말로 설명하자니 댓글로는 너무 길고... 아무튼 두 번 다시 가길 싫은 곳. 캠핑을 20여 년 다녀본 곳 중 제일 기분 나빴던 곳.\\ninput : 별 한 개됴 야깝댜. 왜 샤람들이 별 1개를 쥬는지 겪여본 샤람으료셔 말료 설명햐쟈니 댓글료는 녀뮤 길교... 야뮤튼 듀 번 댜시 갸길 싫은 곳. 캠핑을 20어 년 댜너본 곳 중 제일 기분 냐빴던 곳., output : 별 한 개도 아깝다. 왜 사람들이 별 1개를 주는지 겪어본 사람으로서 말로 설명하자니 댓글로는 너무 길고... 아무튼 두 번 다시 가길 싫은 곳. 캠핑을 20여 년 다녀본 곳 중 제일 기분 나빴던 곳.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompted_datasets['prompt'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5b6479f0-2ab1-40ca-b98c-e1589c6ff366",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'beomi/gemma-ko-7b'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ac66ac95-daa5-4a64-a8ef-f63110ed8db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0459f88f81ff438089228c475dda36ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11263 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "  tokenized = tokenizer(examples['prompt'], padding=\"max_length\", truncation=True)\n",
    "  return tokenized\n",
    "\n",
    "tokenized_datasets = prompted_datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    # num_proc=4  # Use 4 CPU cores\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2e34a78e-08ab-4585-82eb-dd4ac0f5a7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['ID', 'input', 'output', 'restored', 'noised', 'prompt', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 11263\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9f03fa8d-c5b4-4aa1-bb80-df27a0cb7d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset = tokenized_datasets.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ae39a894-0ea0-466e-b525-d87dd6241249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['ID', 'input', 'output', 'restored', 'noised', 'prompt', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 10136\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['ID', 'input', 'output', 'restored', 'noised', 'prompt', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 1127\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(split_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb94255-d140-416c-b7cd-e6188818571d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73b53277-4363-4ec1-b810-4763b8c5dd41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14cd5f245e1f4100ae0f3ed637c3f752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/668 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fd94c58d9114b1aa7e236a35dbea3da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "242b1b6f6dd14657baedb3d6122c06cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7e20ce3bf7c4c55bfea80707fce160b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00006.safetensors:   0%|          | 0.00/2.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11243123d0644e50b339ff725cf8cb20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00006.safetensors:   0%|          | 0.00/2.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28fa9b2c544d4b8f9b93e5263077c7d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00006.safetensors:   0%|          | 0.00/2.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d503d6bc963941fda5dd6bc77f5b3574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00006.safetensors:   0%|          | 0.00/2.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bf6feab34e94c388fc117a017c35251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00006.safetensors:   0%|          | 0.00/2.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66455eb0488c476b8ac41fefb203674e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00006.safetensors:   0%|          | 0.00/2.37G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "615e03d13fdd47baa9ad96591192ef60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3fc0010fcd446e0ad447e43a9044060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ADAPTER_MODEL = \"lora_adapter_7b\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\"beomi/gemma-ko-7b\", device_map='auto', torch_dtype=torch.float16)\n",
    "model = PeftModel.from_pretrained(model, ADAPTER_MODEL, \n",
    "                                  is_trainable=True, device_map='auto', torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "468bbbb8-a639-4d99-9f75-ebd05f08d2ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GemmaForCausalLM(\n",
       "      (model): GemmaModel(\n",
       "        (embed_tokens): Embedding(256000, 3072, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-27): 28 x GemmaDecoderLayer(\n",
       "            (self_attn): GemmaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=3072, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=3072, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=3072, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): GemmaMLP(\n",
       "              (gate_proj): Linear(in_features=3072, out_features=24576, bias=False)\n",
       "              (up_proj): Linear(in_features=3072, out_features=24576, bias=False)\n",
       "              (down_proj): Linear(in_features=24576, out_features=3072, bias=False)\n",
       "              (act_fn): GELUActivation()\n",
       "            )\n",
       "            (input_layernorm): GemmaRMSNorm((3072,), eps=1e-06)\n",
       "            (post_attention_layernorm): GemmaRMSNorm((3072,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): GemmaRMSNorm((3072,), eps=1e-06)\n",
       "        (rotary_emb): GemmaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3072, out_features=256000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6dc8e85-1a9f-41ef-bd7f-8c44fcea5585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Parameters: 3,211,264\n",
      "NOT Trainable Parameters: 8,537,680,896\n"
     ]
    }
   ],
   "source": [
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "\n",
    "non_trainable_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "print(f\"NOT Trainable Parameters: {non_trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89615217-1428-47ab-a932-0d729743519a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight:\n",
      "tensor([[ 0.0196, -0.0005,  0.0051,  ..., -0.0146, -0.0170,  0.0043],\n",
      "        [-0.0118, -0.0008,  0.0108,  ...,  0.0160, -0.0115, -0.0055],\n",
      "        [ 0.0006,  0.0159,  0.0060,  ...,  0.0022,  0.0118,  0.0117],\n",
      "        [ 0.0151,  0.0111, -0.0057,  ..., -0.0030, -0.0148,  0.0075]],\n",
      "       device='cuda:0')\n",
      "\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight:\n",
      "tensor([[ 0.0133, -0.0134, -0.0107,  ...,  0.0104, -0.0093,  0.0022],\n",
      "        [-0.0066, -0.0068, -0.0155,  ..., -0.0019,  0.0063,  0.0136],\n",
      "        [-0.0110, -0.0045, -0.0050,  ..., -0.0138,  0.0077, -0.0079],\n",
      "        [ 0.0159, -0.0320, -0.0012,  ..., -0.0027, -0.0076,  0.0043]],\n",
      "       device='cuda:0')\n",
      "\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight:\n",
      "tensor([[ 0.0058, -0.0134,  0.0018,  ...,  0.0064, -0.0117,  0.0095],\n",
      "        [ 0.0062, -0.0234,  0.0032,  ..., -0.0149, -0.0008, -0.0170],\n",
      "        [-0.0028,  0.0039, -0.0061,  ..., -0.0126,  0.0062, -0.0027],\n",
      "        [ 0.0050,  0.0103,  0.0092,  ..., -0.0064, -0.0016, -0.0182]],\n",
      "       device='cuda:0')\n",
      "\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight:\n",
      "tensor([[ 0.0219,  0.0181,  0.0140,  ..., -0.0058,  0.0207, -0.0079],\n",
      "        [-0.0247, -0.0215,  0.0071,  ...,  0.0093, -0.0045,  0.0074],\n",
      "        [-0.0026,  0.0234, -0.0027,  ..., -0.0071,  0.0051, -0.0047],\n",
      "        [-0.0019,  0.0260, -0.0041,  ..., -0.0005,  0.0101, -0.0027]],\n",
      "       device='cuda:0')\n",
      "\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight:\n",
      "tensor([[ 0.0080,  0.0320,  0.0116,  ..., -0.0039,  0.0220, -0.0084],\n",
      "        [ 0.0053,  0.0253,  0.0226,  ..., -0.0260,  0.0212,  0.0305],\n",
      "        [ 0.0114, -0.0118,  0.0095,  ...,  0.0150, -0.0092,  0.0065],\n",
      "        [ 0.0159,  0.0030,  0.0232,  ..., -0.0019, -0.0199, -0.0033]],\n",
      "       device='cuda:0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lora_a_weights = {k: v for k, v in model.state_dict().items() if \"lora_A\" in k}\n",
    "\n",
    "for i, (name, weight) in enumerate(lora_a_weights.items()):\n",
    "    print(f\"{name}:\\n{weight[:5]}\\n\")  \n",
    "    if i >= 4:  \n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e54efbbd-0bc7-48ce-941e-b2a0529fe792",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    # target_modules = [\n",
    "    #     \"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\",\n",
    "    #     \"gate_proj\", \"down_proj\", \"up_proj\"\n",
    "    # ],\n",
    "    # init_lora_weights = False,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    r=4,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "607f95fd-0891-420d-b48b-d9f8f1d585b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=8,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    eval_strategy=\"epoch\", # epoch 마다 eval\n",
    "    eval_steps=100, # 모델의 평가 주기\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    warmup_steps=10, # 학습률 스케줄링\n",
    "    logging_strategy=\"epoch\", # epoch 마다 log\n",
    "    learning_rate=2e-4,\n",
    "    group_by_length=True,\n",
    "    fp16=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f4aa624e-677b-4703-ad10-d1e089ddb250",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:381: UserWarning: You passed a dataset that is already processed (contains an `input_ids` field) together with a formatting function. Therefore `formatting_func` will be ignored. Either remove the `formatting_func` or pass a dataset that is not already processed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46ba278749ad4f92a6f4037becf0a034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML:   0%|          | 0/10136 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9919d14166ce4c49b9a70ba2f766a8e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/10136 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc892ea9656643ce84661f0483be25e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/10136 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cedd8e128f7410ea688e1a728585501",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting eval dataset to ChatML:   0%|          | 0/1127 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0b03859fb854549980ec90606c36ea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to eval dataset:   0%|          | 0/1127 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05a694dbc90c4862a9d9ba44d2daee0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/1127 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=split_dataset['train'],\n",
    "    eval_dataset=split_dataset['test'],\n",
    "    args=training_args,\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=lambda x: x['input_ids'],\n",
    "    # compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b1061574-ec8a-40ef-b415-387506567332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='790' max='790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [790/790 43:19, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.788200</td>\n",
       "      <td>0.774416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.728700</td>\n",
       "      <td>0.756229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.704900</td>\n",
       "      <td>0.750294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.671500</td>\n",
       "      <td>0.738431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=790, training_loss=0.7160028771509098, metrics={'train_runtime': 2606.6383, 'train_samples_per_second': 19.443, 'train_steps_per_second': 0.303, 'total_flos': 1.0084180393343386e+18, 'train_loss': 0.7160028771509098})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a106b1-0467-47d7-8679-7c161710a5b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6f0a48a4-16df-49fd-bad8-5cecb828c06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADAPTER_MODEL = \"lora_adapter_7b_2\"\n",
    "trainer.model.save_pretrained(ADAPTER_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c11d47c5-7a49-425f-821c-76ebde82be0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed2f68135dee426db17f6c9318b31eae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"beomi/gemma-ko-7b\", device_map='auto', torch_dtype=torch.float16)\n",
    "model = PeftModel.from_pretrained(model, ADAPTER_MODEL, device_map='auto', torch_dtype=torch.float16)\n",
    "model.save_pretrained('finetune_weight_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "03673de6-eb6a-4194-8a9e-d6ba7ad4e1e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04cf29cf767b413fbe976994b610b975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "FINETUNE_MODEL = \"./finetune_weight_2\"\n",
    "\n",
    "finetune_model = AutoModelForCausalLM.from_pretrained(\n",
    "    FINETUNE_MODEL, device_map= {\"\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9ab3d18d-9d64-40f9-ad8a-b760311a868e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=finetune_model,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "22322d76-2062-4819-ac08-146fe2d50a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_to_prompt(query):\n",
    "  messages = [\n",
    "      {\n",
    "          \"role\": \"system\",\n",
    "          \"content\": (\n",
    "              \"You are a helpful assistant specializing in restoring obfuscated Korean reviews. \"\n",
    "              \"Your task is to transform the given obfuscated Korean review into a clear, correct, \"\n",
    "              \"and natural-sounding Korean review that reflects its original meaning. \"\n",
    "              \"Spacing and word length in the output must be restored to the same as in the input. \"\n",
    "              \"Do not provide any description. Print only in Korean.\"\n",
    "          )\n",
    "      },\n",
    "      {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": f\"input : {query}, output : \"\n",
    "      },\n",
    "  ]\n",
    "\n",
    "  prompt = \"\\n\".join([m[\"content\"] for m in messages]).strip()\n",
    "\n",
    "  return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "067dba2f-7dad-45d4-9457-5aac39d18f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_review(query, query_len):\n",
    "  prompt = query_to_prompt(query)\n",
    "\n",
    "  outputs = pipe(\n",
    "      prompt,\n",
    "      do_sample=True,\n",
    "      temperature=0.2,\n",
    "      top_p=0.9,\n",
    "      max_new_tokens=len(query),\n",
    "      eos_token_id=pipe.tokenizer.eos_token_id\n",
    "  )\n",
    "\n",
    "  generated_text = outputs[0]['generated_text']\n",
    "  print(generated_text)\n",
    "  result = generated_text[len(prompt):].strip()\n",
    "\n",
    "  # clean\n",
    "  result = result.split(\"'\")[0]\n",
    "  result = result.split(\"\\n\")[0]\n",
    "  result = result[:query_len]\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d8d84fff-edf5-4396-b6b9-e78ae95f5f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant specializing in restoring obfuscated Korean reviews. Your task is to transform the given obfuscated Korean review into a clear, correct, and natural-sounding Korean review that reflects its original meaning. Spacing and word length in the output must be restored to the same as in the input. Do not provide any description. Print only in Korean.\n",
      "input : 윔취랗 돛심 부꺄 좋앗뎐 효뗄, output :\n"
     ]
    }
   ],
   "source": [
    "query = dataset['train']['input'][25]\n",
    "query_len = len(dataset['train']['input'][25])\n",
    "#query = dataset['train']['input'][10]\n",
    "#query_len = len(dataset['train']['input'][10])\n",
    "prompt = query_to_prompt(query)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "97a4e040-4b6b-41b1-948f-627a8bbc80b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant specializing in restoring obfuscated Korean reviews. Your task is to transform the given obfuscated Korean review into a clear, correct, and natural-sounding Korean review that reflects its original meaning. Spacing and word length in the output must be restored to the same as in the input. Do not provide any description. Print only in Korean.\n",
      "input : 윔취랗 돛심 부꺄 좋앗뎐 효뗄, output : 위치랑 도시 뷰가 좋았던 호텔\n",
      "input : 위치랑\n",
      "위치랑 도시 뷰가 좋았던 호텔\n"
     ]
    }
   ],
   "source": [
    "result = restore_review(query, query_len)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02873374-887e-4c93-a700-20f5d3ef737c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
