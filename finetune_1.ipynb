{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0450db41-30a1-43b8-a339-cb9c507702de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files removed: 40\n"
     ]
    }
   ],
   "source": [
    "!pip cache purge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea3cfcb3-333e-4c50-a9c2-0bca5a2a9f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip uninstall -y bitsandbytes\n",
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f845242-e82f-4bc0-86dc-b3d8fd1f330e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install datasets\n",
    "!pip install accelerate\n",
    "!pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "089038b5-0713-4671-847e-64ded0de2c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "398aa9b2-998a-4401-ba9a-887108cbccda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "!pip install --upgrade typing_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c9d97747-89aa-4443-8670-50e3ab731304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "de4d946d-28ba-4015-bf68-715362386d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "988c1195-14e0-4358-b519-bd07240e0b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c2f741c9-6b31-46f1-af00-7e9958373da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"csv\", data_files='./train.csv', encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8185f38c-1a22-4ea4-b3de-3895b82980d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['ID', 'input', 'output'],\n",
      "        num_rows: 11263\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "17c6c6d4-a176-4dc3-9e28-975df560c394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(examples):\n",
    "  messages = [\n",
    "      {\n",
    "          \"role\": \"system\",\n",
    "          \"content\": (\n",
    "              \"You are a helpful assistant specializing in restoring obfuscated Korean reviews. \"\n",
    "              \"Your task is to transform the given obfuscated Korean review into a clear, correct, \"\n",
    "              \"and natural-sounding Korean review that reflects its original meaning. \"\n",
    "              \"Spacing and word length in the output must be restored to the same as in the input. \"\n",
    "              \"Do not provide any description. Print only in Korean.\"\n",
    "          )\n",
    "      },\n",
    "      {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": f\"input : {examples['input']}, output : {examples['output']}\"\n",
    "      },\n",
    "  ]\n",
    "\n",
    "  prompt = \"\\n\".join([ # f\"<start_of_turn> Role : {m['role']}, \\n {m['content']} <end_of_turn>\"\n",
    "                      m['content']\n",
    "                      # f\"<start_of_turn>{m['content']}<end_of_turn>\"\n",
    "                      for m in messages]\n",
    "                    ).strip()\n",
    "\n",
    "  return {\"prompt\": prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8ce022fe-8efd-4cc6-962a-9b56d91120a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompted_datasets = dataset[\"train\"].map(\n",
    "    generate_prompt,\n",
    "    # remove_columns=dataset[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "64f9c3b3-1f8f-4180-a187-a82323bc9be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['ID', 'input', 'output', 'prompt'],\n",
      "    num_rows: 11263\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(prompted_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cc2f1c50-066a-46ef-89c3-9a61d0c2b423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a helpful assistant specializing in restoring obfuscated Korean reviews. Your task is to transform the given obfuscated Korean review into a clear, correct, and natural-sounding Korean review that reflects its original meaning. Spacing and word length in the output must be restored to the same as in the input. Do not provide any description. Print only in Korean.\\ninput : 별 한 게토 았깝땀. 왜 싸람듯릭 펼 1캐를 쥰눈징 컥꺾폰 싸람믐롯섞 맒록 섧멍핥쟈닐 탯끎룐눈 녀뮤 퀼교... 야뭍툰 둠 변 닺씨 깍낄 싫훈 굣. 깸삥읊 20여 년 댜녁뵨 곧 중 쩨윌 귑푼 낙팠떤 곶., output : 별 한 개도 아깝다. 왜 사람들이 별 1개를 주는지 겪어본 사람으로서 말로 설명하자니 댓글로는 너무 길고... 아무튼 두 번 다시 가길 싫은 곳. 캠핑을 20여 년 다녀본 곳 중 제일 기분 나빴던 곳.'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompted_datasets['prompt'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e4acce6e-d9c0-4bc2-9683-0d7fcbd2f346",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'beomi/gemma-ko-7b'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b6ae0b99-0e03-461e-b369-98d0013b1836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "  tokenized = tokenizer(examples['prompt'], padding=\"max_length\", truncation=True)\n",
    "  return tokenized\n",
    "\n",
    "tokenized_datasets = prompted_datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    # num_proc=4  # Use 4 CPU cores\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f23463da-a09e-42cf-830a-053d1ee678e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['ID', 'input', 'output', 'prompt', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 11263\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c59f4fc9-fef3-48a1-9d3c-75393cd3613e",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset = tokenized_datasets.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f4a97ff7-18f9-4f85-a4f1-041f366a2704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['ID', 'input', 'output', 'prompt', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 10136\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['ID', 'input', 'output', 'prompt', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 1127\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(split_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d0a0180-3bb9-44a4-9e8e-16be1d12a284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8cdf5f89c084dc98e8c14c685fafe7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74e47a966bf8413785f7e8233cef952b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0b5490b4599405ca028bf2e1a45a934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00006.safetensors:   0%|          | 0.00/2.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56f3cb42487d45b4a7257cc3f87a66a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00006.safetensors:   0%|          | 0.00/2.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11f549e00c4a476fa52666fa8d9b1586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00006.safetensors:   0%|          | 0.00/2.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fe559a284a04694b10c69a970f3d120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00006.safetensors:   0%|          | 0.00/2.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4e8499ed391460d8cd9d6e1af794475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00006.safetensors:   0%|          | 0.00/2.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "429cc0ae735440a2a76d53afa4cb9709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00006.safetensors:   0%|          | 0.00/2.37G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a298d23b4e5d4d7cb3c3d877081ff68a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaf4835d36dd4f88b223c78dd717d2e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type= 'nf4',\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "model_id = 'beomi/gemma-ko-7b'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config = bnb_config, device_map={\"\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d67fab2e-8381-4872-a6f7-b11d9d95f62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Parameters: 786,607,104\n",
      "NOT Trainable Parameters: 3,875,536,896\n"
     ]
    }
   ],
   "source": [
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "\n",
    "non_trainable_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "print(f\"NOT Trainable Parameters: {non_trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17b960d1-673b-41b0-a550-ef0bf03f4ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    # target_modules = [\n",
    "    #     \"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\",\n",
    "    #     \"gate_proj\", \"down_proj\", \"up_proj\"\n",
    "    # ],\n",
    "    # init_lora_weights = False,\n",
    "    # lora_alpha=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    # r=64,\n",
    "    # r=8,\n",
    "    r=4,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9c4b4e8-28f5-4e5c-886f-2abff84c58d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9cdce235-46db-4e29-b7c9-67b98a26ef5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GemmaForCausalLM(\n",
       "      (model): GemmaModel(\n",
       "        (embed_tokens): Embedding(256000, 3072, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-27): 28 x GemmaDecoderLayer(\n",
       "            (self_attn): GemmaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): GemmaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=3072, out_features=24576, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=3072, out_features=24576, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=24576, out_features=3072, bias=False)\n",
       "              (act_fn): GELUActivation()\n",
       "            )\n",
       "            (input_layernorm): GemmaRMSNorm((3072,), eps=1e-06)\n",
       "            (post_attention_layernorm): GemmaRMSNorm((3072,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): GemmaRMSNorm((3072,), eps=1e-06)\n",
       "        (rotary_emb): GemmaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3072, out_features=256000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "afec8c01-9629-4575-9678-cdc4aaf7114b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 0 || all params: 8,540,892,160 || trainable%: 0.0000\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "677ede62-7487-4e33-a9ac-fcf5772156c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    eval_strategy=\"epoch\", # epoch 마다 eval\n",
    "    eval_steps=100, # 모델의 평가 주기\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    warmup_steps=10, # 학습률 스케줄링\n",
    "    logging_strategy=\"epoch\", # epoch 마다 log\n",
    "    learning_rate=2e-4,\n",
    "    group_by_length=True,\n",
    "    fp16=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae3ac8c0-3877-42a9-9b15-15da10989a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:382: UserWarning: You passed a dataset that is already processed (contains an `input_ids` field) together with a formatting function. Therefore `formatting_func` will be ignored. Either remove the `formatting_func` or pass a dataset that is not already processed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e19f56201e154335a17e0e9254e76d53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML:   0%|          | 0/10136 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b591977993c6444383482537e6a718d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/10136 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c346149e3bff46a7a058513b28800cc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/10136 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b42a8b82c7cd48aaa36f79c5bef28aa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting eval dataset to ChatML:   0%|          | 0/1127 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af13b0df3a86454382687cacb98eea80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to eval dataset:   0%|          | 0/1127 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eef029e337d2474ebce8e7b2971ac87e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to eval dataset:   0%|          | 0/1127 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=split_dataset['train'],\n",
    "    eval_dataset=split_dataset['test'],\n",
    "    args=training_args,\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=lambda x: x['input_ids'],\n",
    "    # compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24bbe2a8-ff9a-437d-b118-e1b48dd2c235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1580' max='1580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1580/1580 53:33, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.585600</td>\n",
       "      <td>1.383088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.292100</td>\n",
       "      <td>1.278292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.209900</td>\n",
       "      <td>1.233846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.131100</td>\n",
       "      <td>1.202300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1580, training_loss=1.275977817969986, metrics={'train_runtime': 3218.111, 'train_samples_per_second': 15.748, 'train_steps_per_second': 0.491, 'total_flos': 6.009973377429504e+17, 'train_loss': 1.275977817969986})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a9b6be1-5206-4536-8333-beb85a967443",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADAPTER_MODEL = \"lora_adapter_7b\"\n",
    "trainer.model.save_pretrained(ADAPTER_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a2da4ab6-1972-423a-b5cb-ff5a387779d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc455050daef461ab4070310ece2f830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"beomi/gemma-ko-7b\", device_map='auto', torch_dtype=torch.float16)\n",
    "model = PeftModel.from_pretrained(model, ADAPTER_MODEL, device_map='auto', torch_dtype=torch.float16)\n",
    "model.save_pretrained('finetune_weight_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7ec58953-1405-4da3-bfb0-637ddf3a9738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b75d852e23654785a32bb8c01795041f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "FINETUNE_MODEL = \"./finetune_weight_1\"\n",
    "\n",
    "finetune_model = AutoModelForCausalLM.from_pretrained(\n",
    "    FINETUNE_MODEL, device_map= {\"\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4c02fe84-416f-4251-8c6c-b96823975804",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=finetune_model,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "31d50e45-f474-4e43-a63a-2e7913cc7459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_to_prompt(query):\n",
    "  messages = [\n",
    "      {\n",
    "          \"role\": \"system\",\n",
    "          \"content\": (\n",
    "              \"You are a helpful assistant specializing in restoring obfuscated Korean reviews. \"\n",
    "              \"Your task is to transform the given obfuscated Korean review into a clear, correct, \"\n",
    "              \"and natural-sounding Korean review that reflects its original meaning. \"\n",
    "              \"Spacing and word length in the output must be restored to the same as in the input. \"\n",
    "              \"Do not provide any description. Print only in Korean.\"\n",
    "          )\n",
    "      },\n",
    "      {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": f\"input : {query}, output : \"\n",
    "      },\n",
    "  ]\n",
    "\n",
    "  prompt = \"\\n\".join([m[\"content\"] for m in messages]).strip()\n",
    "\n",
    "  return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1d44c533-fa9a-488a-a50c-b9cd20549226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_review(query, query_len):\n",
    "  prompt = query_to_prompt(query)\n",
    "\n",
    "  outputs = pipe(\n",
    "      prompt,\n",
    "      do_sample=True,\n",
    "      temperature=0.2,\n",
    "      top_p=0.9,\n",
    "      max_new_tokens=len(query),\n",
    "      eos_token_id=pipe.tokenizer.eos_token_id\n",
    "  )\n",
    "\n",
    "  generated_text = outputs[0]['generated_text']\n",
    "  print(generated_text)\n",
    "  result = generated_text[len(prompt):].strip()\n",
    "\n",
    "  # clean\n",
    "  result = result.split(\"'\")[0]\n",
    "  result = result.split(\"\\n\")[0]\n",
    "  result = result[:query_len]\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "83a28483-01b9-460b-a19d-0f8dca79cc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant specializing in restoring obfuscated Korean reviews. Your task is to transform the given obfuscated Korean review into a clear, correct, and natural-sounding Korean review that reflects its original meaning. Spacing and word length in the output must be restored to the same as in the input. Do not provide any description. Print only in Korean.\n",
      "input : 편힘 30퓬 넒계 쉰효 밭았써 긷타린는 쭐 봤욺면써 먈 얀항교 윗딱갸 거우 좌훽전을 받았섞 둘얹갼닙 쭈짰쟝 윕균셔 만짤략곡 팔료 똘럽본냅뉘따. 키타림먼 않돼냘 헤돋 큰낳 타 톨럼뾰냅뉜따. 믿퉤 깆딸린눈 싸람뚤운 젊많끔 냐온뉘 출챠핥 쑤 있욹 컬략꼬 쌩깍깖짐 큭계 따 둘럿카푠찢또 못햐교 구낭 팍큐닿한 짜랍굣 눅까 셍갸칸나굘오. 끓쳇써약 야난튁꼬프예 쥬짯햐쿄 셧틀 탸쿄 욜략교 일악귀합닙닥. 군낭 철음붙떠 셔툴 따눈 촉읏로 않넵를 졺 핫턴찌! 쭈윤 낡 읽항씬는 커 앉수러윤갖 헷는떼 쩐헐... 읾단 씨착쀼떠 읾뮈치 뎁뽁 깜먹교 옥항꼬 십울 정됴롬 윌쩔리 팡쉭위 념뮤 쌍슐만 쮜한련눈 컷잊 누녜 뵤윕닉따. 구리코 눈 내린눈 겁 꼭 앉 봔토 됩닢따. 흩뿔립뜻 뽈뿜엾열셔 큰냥 엎는 궤 칼큼한 눅킴밉뉘댜., output :\n"
     ]
    }
   ],
   "source": [
    "# query = dataset['train']['input'][25]\n",
    "# query_len = len(dataset['train']['input'][25])\n",
    "query = dataset['train']['input'][10]\n",
    "query_len = len(dataset['train']['input'][10])\n",
    "prompt = query_to_prompt(query)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "aff99851-924c-4b73-861b-3bf62a75c993",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant specializing in restoring obfuscated Korean reviews. Your task is to transform the given obfuscated Korean review into a clear, correct, and natural-sounding Korean review that reflects its original meaning. Spacing and word length in the output must be restored to the same as in the input. Do not provide any description. Print only in Korean.\n",
      "input : 편힘 30퓬 넒계 쉰효 밭았써 긷타린는 쭐 봤욺면써 먈 얀항교 윗딱갸 거우 좌훽전을 받았섞 둘얹갼닙 쭈짰쟝 윕균셔 만짤략곡 팔료 똘럽본냅뉘따. 키타림먼 않돼냘 헤돋 큰낳 타 톨럼뾰냅뉜따. 믿퉤 깆딸린눈 싸람뚤운 젊많끔 냐온뉘 출챠핥 쑤 있욹 컬략꼬 쌩깍깖짐 큭계 따 둘럿카푠찢또 못햐교 구낭 팍큐닿한 짜랍굣 눅까 셍갸칸나굘오. 끓쳇써약 야난튁꼬프예 쥬짯햐쿄 셧틀 탸쿄 욜략교 일악귀합닙닥. 군낭 철음붙떠 셔툴 따눈 촉읏로 않넵를 졺 핫턴찌! 쭈윤 낡 읽항씬는 커 앉수러윤갖 헷는떼 쩐헐... 읾단 씨착쀼떠 읾뮈치 뎁뽁 깜먹교 옥항꼬 십울 정됴롬 윌쩔리 팡쉭위 념뮤 쌍슐만 쮜한련눈 컷잊 누녜 뵤윕닉따. 구리코 눈 내린눈 겁 꼭 앉 봔토 됩닢따. 흩뿔립뜻 뽈뿜엾열셔 큰냥 엎는 궤 칼큼한 눅킴밉뉘댜., output : 편히 30분 넓게 쉬려고 기다리는 줄 봤으면서 말 안하고 있다가 거의 자리가 났을 때 뛰어보냅니다. 기다리면 안된다 해도 그냥 다 뛰어보냅니다. 믿을 수 있을까 나오니까 주차할 수 있을 거라고 생각할 겨 같게 다 뛰어가보지도 못하고 그냥 박구당한 차라고 누가 생각하나요. 글쎄써야 아난티코브에 주차하고 셔틀 타고 올라고 이야기합니다. 그냥 처음부터 셔틀 다는 쪽으로 안내를 좀 하던지! 주운 날 일하시는 거 안쓰러운가 했는데 전혀... 일단 시착부터 이미지 대표 감먹고 욕하고 싶을 정도로 일처리 빵식이 너무 상술만 찌한려는 것이 눈에 보입니다. 그리고 눈 내리는 거 꼭 안 봐도 됩니다. 허브리듯 폭풍여서 그냥 없는 게 깔끔한 느낌입니다.\n",
      "편히 30분 넓게 쉬려고 기다리는 줄 봤으면서 말 안하고 있다가 거의 자리가 났을 때 뛰어보냅니다. 기다리면 안된다 해도 그냥 다 뛰어보냅니다. 믿을 수 있을까 나오니까 주차할 수 있을 거라고 생각할 겨 같게 다 뛰어가보지도 못하고 그냥 박구당한 차라고 누가 생각하나요. 글쎄써야 아난티코브에 주차하고 셔틀 타고 올라고 이야기합니다. 그냥 처음부터 셔틀 다는 쪽으로 안내를 좀 하던지! 주운 날 일하시는 거 안쓰러운가 했는데 전혀... 일단 시착부터 이미지 대표 감먹고 욕하고 싶을 정도로 일처리 빵식이 너무 상술만 찌한려는 것이 눈에 보입니다. 그리고 눈 내리는 거 꼭 안 봐도 됩니다. 허브리듯 폭풍여서 그냥 없는 게 깔끔한 느낌입니다.\n"
     ]
    }
   ],
   "source": [
    "result = restore_review(query, query_len)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "85c43370-9b85-463a-8b56-94964a301a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "편히 30분 넓게 쉬려고 기다리는 줄 봤으면서 말 안하고 있다가 거의 자리가 났을 때 뛰어보냅니다. 기다리면 안된다 해도 그냥 다 뛰어보냅니다. 믿을 수 있을까 나오니까 주차할 수 있을 거라고 생각할 겨 같게 다 뛰어가보지도 못하고 그냥 박구당한 차라고 누가 생각하나요. 글쎄써야 아난티코브에 주차하고 셔틀 타고 올라고 이야기합니다. 그냥 처음부터 셔틀 다는 쪽으로 안내를 좀 하던지! 주운 날 일하시는 거 안쓰러운가 했는데 전혀... 일단 시착부터 이미지 대표 감먹고 욕하고 싶을 정도로 일처리 빵식이 너무 상술만 찌한려는 것이 눈에 보입니다. 그리고 눈 내리는 거 꼭 안 봐도 됩니다. 허브리듯 폭풍여서 그냥 없는 게 깔끔한 느낌입니다.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7ece33-4756-4931-bedd-ccb5a1388d3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbc3949-e95f-4c29-b091-b2b1bd78252d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940b5498-c15b-462e-b73c-a4f7f4b39da6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
