{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c59f2d9-ad35-4016-b773-f600603b7e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files removed: 236\n"
     ]
    }
   ],
   "source": [
    "!pip cache purge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16007afe-d9d7-47f3-bc5a-b7ff566251c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip uninstall -y bitsandbytes\n",
    "!pip install -U bitsandbytes\n",
    "\n",
    "!pip install datasets\n",
    "!pip install accelerate\n",
    "!pip install peft\n",
    "\n",
    "!pip install -U trl\n",
    "!pip install --upgrade typing_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83d5f06c-43e0-4172-821d-fccf6499084c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from peft import PeftModel\n",
    "\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c18116a3-4f87-4220-b01c-0b4735241c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6824ddd9-a27d-4236-98b6-17aeaf9f9e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated Memory: 0.00 MB\n",
      "Cached Memory: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Allocated Memory: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "print(f\"Cached Memory: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a35cf2e4-8dbc-48b5-995c-5563a9e17be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"csv\", data_files='./train.csv', encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e614dbd0-1cfd-49ce-ba13-d8520e275232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['ID', 'input', 'output'],\n",
      "        num_rows: 11263\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbfd702d-2c0d-4d7a-a970-2da10fc3b6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(examples):\n",
    "  messages = [\n",
    "      f\"input : {examples['input']}, output : {examples['output']}\",\n",
    "  ]\n",
    "\n",
    "  prompt = \"\\n\".join([m for m in messages]).strip()\n",
    "\n",
    "  return {\"prompt\": prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05f663a9-f95c-451c-8cb7-9b28edff002c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompted_datasets = dataset[\"train\"].map(\n",
    "    generate_prompt,\n",
    "    # remove_columns=dataset[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4705a250-48a6-4a6b-b242-ce7c57563ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['ID', 'input', 'output', 'prompt'],\n",
      "    num_rows: 11263\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(prompted_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46503d62-524b-4b08-bf7e-fe8f2bf142c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'input : 별 한 게토 았깝땀. 왜 싸람듯릭 펼 1캐를 쥰눈징 컥꺾폰 싸람믐롯섞 맒록 섧멍핥쟈닐 탯끎룐눈 녀뮤 퀼교... 야뭍툰 둠 변 닺씨 깍낄 싫훈 굣. 깸삥읊 20여 년 댜녁뵨 곧 중 쩨윌 귑푼 낙팠떤 곶., output : 별 한 개도 아깝다. 왜 사람들이 별 1개를 주는지 겪어본 사람으로서 말로 설명하자니 댓글로는 너무 길고... 아무튼 두 번 다시 가길 싫은 곳. 캠핑을 20여 년 다녀본 곳 중 제일 기분 나빴던 곳.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompted_datasets['prompt'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d140d175-acb4-48d2-83d8-bf00dc02cd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'beomi/gemma-ko-7b'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aca667ab-4705-46bf-8a00-e4fbd1b34000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "  tokenized = tokenizer(examples['prompt'], padding=\"max_length\", truncation=True)\n",
    "  return tokenized\n",
    "\n",
    "tokenized_datasets = prompted_datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    # num_proc=4  # Use 4 CPU cores\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac1656c0-e343-475a-b7ae-ee2a8a89be09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['ID', 'input', 'output', 'prompt', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 11263\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84e352e0-7cd9-444d-9fcf-5c3c0f0f854b",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset = tokenized_datasets.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2c341ee-7784-4fb6-9cb8-db0f0f2d5ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['ID', 'input', 'output', 'prompt', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 10136\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['ID', 'input', 'output', 'prompt', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 1127\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(split_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee80757d-ed5c-4e27-af02-68ae17a549c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ef7396f-a439-44c2-b77d-0af6b2baa10c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de43bc0d1b2b4df0a82d91ef4ff95155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ADAPTER_MODEL = \"lora_adapter_7b_2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\"beomi/gemma-ko-7b\", device_map='auto', torch_dtype=torch.float16)\n",
    "model = PeftModel.from_pretrained(model, ADAPTER_MODEL, \n",
    "                                  is_trainable=True, device_map='auto', torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "414dfb97-63bb-4bac-99ac-1cf489667f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Parameters: 3,211,264\n",
      "NOT Trainable Parameters: 8,537,680,896\n"
     ]
    }
   ],
   "source": [
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "\n",
    "non_trainable_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "print(f\"NOT Trainable Parameters: {non_trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80454a80-6681-4d45-a39f-3bc530245e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    target_modules = [\"q_proj\", \"k_proj\"],\n",
    "    # [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    # target_modules = [\n",
    "    #     \"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\",\n",
    "    #     \"gate_proj\", \"down_proj\", \"up_proj\"\n",
    "    # ],\n",
    "    # init_lora_weights = False,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    r=4,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cea31323-bbbd-45fe-964c-5f2e510913fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    eval_strategy=\"epoch\", # epoch 마다 eval\n",
    "    eval_steps=100, # 모델의 평가 주기\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    warmup_steps=10, # 학습률 스케줄링\n",
    "    logging_strategy=\"epoch\", # epoch 마다 log\n",
    "    learning_rate=2e-4,\n",
    "    group_by_length=True,\n",
    "    fp16=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48e7b3fe-f8be-4dec-9377-309f8f447f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:381: UserWarning: You passed a dataset that is already processed (contains an `input_ids` field) together with a formatting function. Therefore `formatting_func` will be ignored. Either remove the `formatting_func` or pass a dataset that is not already processed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36ed3d9a574d4cadb9dcb44b8aec51ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML:   0%|          | 0/10136 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b1c4ad87a4847ec8182cd38de13f849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/10136 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0607d084b4814bb7b5b590efa2e31cf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/10136 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "127c31d20a374943a4989c13a3b53f10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting eval dataset to ChatML:   0%|          | 0/1127 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96f146feb5ba47c38d12ab34bd8fb089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to eval dataset:   0%|          | 0/1127 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c55fa04121c477f8895be2499972dc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/1127 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=split_dataset['train'],\n",
    "    eval_dataset=split_dataset['test'],\n",
    "    args=training_args,\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=lambda x: x['input_ids'],\n",
    "    # compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b680d08a-e845-4289-b41e-89d01e775399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1580' max='1580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1580/1580 27:08, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.738600</td>\n",
       "      <td>1.778242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.678200</td>\n",
       "      <td>1.757370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.616100</td>\n",
       "      <td>1.741204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.505200</td>\n",
       "      <td>1.739605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1580, training_loss=1.61899427582946, metrics={'train_runtime': 1632.2358, 'train_samples_per_second': 31.049, 'train_steps_per_second': 0.968, 'total_flos': 4.324872732191539e+17, 'train_loss': 1.61899427582946})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02be339d-3839-4f86-8dae-9f6c717f72e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af02fceb-5840-42e3-ac30-268935900097",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADAPTER_MODEL = \"lora_adapter_7b_3\"\n",
    "trainer.model.save_pretrained(ADAPTER_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03a90c74-c47c-495b-8d07-0cbcbeee84dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py:1536: UserWarning: Current model requires 256 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a55dc83a0d7d42f682216536e52da753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py:1536: UserWarning: Current model requires 512 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"beomi/gemma-ko-7b\", device_map='auto', torch_dtype=torch.float16)\n",
    "model = PeftModel.from_pretrained(model, ADAPTER_MODEL, device_map='auto', torch_dtype=torch.float16)\n",
    "model.save_pretrained('finetune_weight_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fff5956e-fcc0-4d14-91cb-af7dc79134f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ee41d7a70d94fba9117f38e75e5561a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "FINETUNE_MODEL = \"./finetune_weight_3\"\n",
    "\n",
    "finetune_model = AutoModelForCausalLM.from_pretrained(\n",
    "    FINETUNE_MODEL, device_map= {\"\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0902ad8f-052d-4665-b335-052a0c62bfdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=finetune_model,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34b6b6c7-c2ab-4b08-bb9c-e7c7295c35d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_to_prompt(query):\n",
    "  messages = [\n",
    "      {\n",
    "          \"role\": \"system\",\n",
    "          \"content\": (\n",
    "              \"You are a helpful assistant specializing in restoring obfuscated Korean reviews. \"\n",
    "              \"Your task is to transform the given obfuscated Korean review into a clear, correct, \"\n",
    "              \"and natural-sounding Korean review that reflects its original meaning. \"\n",
    "              \"Spacing and word length in the output must be restored to the same as in the input. \"\n",
    "              \"Do not provide any description. Print only in Korean.\"\n",
    "          )\n",
    "      },\n",
    "      {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": f\"input : {query}, output : \"\n",
    "      },\n",
    "  ]\n",
    "\n",
    "  prompt = \"\\n\".join([m[\"content\"] for m in messages]).strip()\n",
    "\n",
    "  return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f5f18af-114b-4ba3-9ac6-f67200bb8a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_review(query, query_len):\n",
    "  prompt = query_to_prompt(query)\n",
    "\n",
    "  outputs = pipe(\n",
    "      prompt,\n",
    "      do_sample=True,\n",
    "      temperature=0.2,\n",
    "      top_p=0.9,\n",
    "      max_new_tokens=len(query),\n",
    "      eos_token_id=pipe.tokenizer.eos_token_id\n",
    "  )\n",
    "\n",
    "  generated_text = outputs[0]['generated_text']\n",
    "  print(generated_text)\n",
    "  result = generated_text[len(prompt):].strip()\n",
    "\n",
    "  # clean\n",
    "  result = result.split(\"'\")[0]\n",
    "  result = result.split(\"\\n\")[0]\n",
    "  result = result[:query_len]\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "433174e9-ccf6-4316-92d6-d24d17e742fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant specializing in restoring obfuscated Korean reviews. Your task is to transform the given obfuscated Korean review into a clear, correct, and natural-sounding Korean review that reflects its original meaning. Spacing and word length in the output must be restored to the same as in the input. Do not provide any description. Print only in Korean.\n",
      "input : 편힘 30퓬 넒계 쉰효 밭았써 긷타린는 쭐 봤욺면써 먈 얀항교 윗딱갸 거우 좌훽전을 받았섞 둘얹갼닙 쭈짰쟝 윕균셔 만짤략곡 팔료 똘럽본냅뉘따. 키타림먼 않돼냘 헤돋 큰낳 타 톨럼뾰냅뉜따. 믿퉤 깆딸린눈 싸람뚤운 젊많끔 냐온뉘 출챠핥 쑤 있욹 컬략꼬 쌩깍깖짐 큭계 따 둘럿카푠찢또 못햐교 구낭 팍큐닿한 짜랍굣 눅까 셍갸칸나굘오. 끓쳇써약 야난튁꼬프예 쥬짯햐쿄 셧틀 탸쿄 욜략교 일악귀합닙닥. 군낭 철음붙떠 셔툴 따눈 촉읏로 않넵를 졺 핫턴찌! 쭈윤 낡 읽항씬는 커 앉수러윤갖 헷는떼 쩐헐... 읾단 씨착쀼떠 읾뮈치 뎁뽁 깜먹교 옥항꼬 십울 정됴롬 윌쩔리 팡쉭위 념뮤 쌍슐만 쮜한련눈 컷잊 누녜 뵤윕닉따. 구리코 눈 내린눈 겁 꼭 앉 봔토 됩닢따. 흩뿔립뜻 뽈뿜엾열셔 큰냥 엎는 궤 칼큼한 눅킴밉뉘댜., output :\n"
     ]
    }
   ],
   "source": [
    "# query = dataset['train']['input'][25]\n",
    "# query_len = len(dataset['train']['input'][25])\n",
    "query = dataset['train']['input'][10]\n",
    "query_len = len(dataset['train']['input'][10])\n",
    "prompt = query_to_prompt(query)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b8c8708-6bcd-4b68-989c-c87e8c4d7164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant specializing in restoring obfuscated Korean reviews. Your task is to transform the given obfuscated Korean review into a clear, correct, and natural-sounding Korean review that reflects its original meaning. Spacing and word length in the output must be restored to the same as in the input. Do not provide any description. Print only in Korean.\n",
      "input : 편힘 30퓬 넒계 쉰효 밭았써 긷타린는 쭐 봤욺면써 먈 얀항교 윗딱갸 거우 좌훽전을 받았섞 둘얹갼닙 쭈짰쟝 윕균셔 만짤략곡 팔료 똘럽본냅뉘따. 키타림먼 않돼냘 헤돋 큰낳 타 톨럼뾰냅뉜따. 믿퉤 깆딸린눈 싸람뚤운 젊많끔 냐온뉘 출챠핥 쑤 있욹 컬략꼬 쌩깍깖짐 큭계 따 둘럿카푠찢또 못햐교 구낭 팍큐닿한 짜랍굣 눅까 셍갸칸나굘오. 끓쳇써약 야난튁꼬프예 쥬짯햐쿄 셧틀 탸쿄 욜략교 일악귀합닙닥. 군낭 철음붙떠 셔툴 따눈 촉읏로 않넵를 졺 핫턴찌! 쭈윤 낡 읽항씬는 커 앉수러윤갖 헷는떼 쩐헐... 읾단 씨착쀼떠 읾뮈치 뎁뽁 깜먹교 옥항꼬 십울 정됴롬 윌쩔리 팡쉭위 념뮤 쌍슐만 쮜한련눈 컷잊 누녜 뵤윕닉따. 구리코 눈 내린눈 겁 꼭 앉 봔토 됩닢따. 흩뿔립뜻 뽈뿜엾열셔 큰냥 엎는 궤 칼큼한 눅킴밉뉘댜., output : 편히 30분 넘게 신호 받아서 기다리는 줄 봤으면서 말 안하고 있다가 겨우 자리를 받아서 들어가니 주차장 입구서 만차라고 바로 돌려보냅니다. 기다리면 안되나 해도 그냥 다 돌려보냅니다. 미친 기다리는 사람들은 잠깐 밖으로 나오니 주차할 수 있을 거라고 생각할 거라고 쌩각할지 크게 다 둘러가보지도 못하고 그냥 빠꾸당한 차라고 느까 생각하나요. 그쳐서야 아난티코브에 주차하고 셔틀 타고 오라고 이야기합니다. 그냥 처음부터 셔틀 타는 쪽으로 안내를 좀 하던지! 추운 날 일하시는 거 안쓰러운가 했는데 전혀... 일단 시착부터 이미지 대표 깜먹고 욕하고 싶을 정도로 일처리 빡시의 너무 상술만 취하려는 것이 눈에 보입니다. 그리고 눈 내리는 거 꼭 안 봐도 됩니다. 흩뿌리듯 뿌옇어서 그냥 없는 게 깔끔한 느낌입니다.\n",
      "췬졀햐계 쭙땔 쭝 쭙쳔햐눈 슉쏘 쭝 쵯샹위 슉쏘 쭙쳔합뉘댜. 췬졀햐계 춥댈 쭝 춥천햐눈 쑥쏘 중 쵯샹위 쑥쏘 춥천합니댜\n",
      "편히 30분 넘게 신호 받아서 기다리는 줄 봤으면서 말 안하고 있다가 겨우 자리를 받아서 들어가니 주차장 입구서 만차라고 바로 돌려보냅니다. 기다리면 안되나 해도 그냥 다 돌려보냅니다. 미친 기다리는 사람들은 잠깐 밖으로 나오니 주차할 수 있을 거라고 생각할 거라고 쌩각할지 크게 다 둘러가보지도 못하고 그냥 빠꾸당한 차라고 느까 생각하나요. 그쳐서야 아난티코브에 주차하고 셔틀 타고 오라고 이야기합니다. 그냥 처음부터 셔틀 타는 쪽으로 안내를 좀 하던지! 추운 날 일하시는 거 안쓰러운가 했는데 전혀... 일단 시착부터 이미지 대표 깜먹고 욕하고 싶을 정도로 일처리 빡시의 너무 상술만 취하려는 것이 눈에 보입니다. 그리고 눈 내리는 거 꼭 안 봐도 됩니다. 흩뿌리듯 뿌옇어서 그냥 없는 게 깔끔\n"
     ]
    }
   ],
   "source": [
    "result = restore_review(query, query_len)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0b1a9f-512e-4e04-ad93-3e57e4e0d476",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
